local bert_model = "/home/hetao/Data/pre_train/chinese_roberta_wwm_ext_pytorch/vocab.txt";
local bert_archive = "/home/hetao/Data/pre_train/chinese_roberta_wwm_ext_pytorch/chinese_roberta_wwm_ext_pytorch.tar.gz";

{
    "dataset_reader": {
        "lazy": false,
        "type": "text_classification_txt",
        "tokenizer": {
          "type":"word",
          "word_splitter":{
            "type":"bert-basic"
            }
        },
        "token_indexers": {
          "bert":{
            "type":"bert-pretrained",
            "pretrained_model": bert_model
            }
        },
        "testing": false
},
    "train_data_path": "./data/train.csv",
    "validation_data_path": "./data/test.csv",
    "model": {
        "type": "text_classifier_fscore_focal_loss",
        "text_field_embedder": {
            "allow_unmatched_keys": true,
            "embedder_to_indexer_map": {
                "bert": ["bert", "bert-offsets"],
            },
            "token_embedders": {
                "bert": {
                    "type": "bert-pretrained",
                    "pretrained_model": bert_archive,
                    "top_layer_only": false,
                    "requires_grad": false
                }
            }
        },
        "seq2vec_encoder": {
           "type": "bert_pooler",
           "pretrained_model": bert_archive,
           "requires_grad": false
        },
        "dropout": 0.25,
        "loss":"cross_entropy_loss"
},
    "iterator": {
        "type": "bucket",
        "sorting_keys": [["tokens", "num_tokens"]],
        "batch_size": 5
    },
    "trainer": {
        "optimizer": {
            "type": "adam",
            "lr": 0.001
        },
        "validation_metric": "+accuracy",
        "num_serialized_models_to_keep": 1,
        "num_epochs": 15,
        "grad_norm": 10.0,
        "patience": 5,
        "cuda_device": 0
    }
}